# Crawl4AI é›†æˆæŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨Financial Data Collectorç³»ç»Ÿä¸­ä½¿ç”¨Crawl4AIè¿›è¡Œé«˜çº§ç½‘é¡µçˆ¬å–ã€‚

## ğŸš€ ä»€ä¹ˆæ˜¯Crawl4AI

Crawl4AIæ˜¯ä¸€ä¸ªä¸“ä¸ºAIåº”ç”¨è®¾è®¡çš„å¼€æºPythonåº“ï¼Œæ—¨åœ¨ç®€åŒ–ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–è¿‡ç¨‹ã€‚å®ƒæä¾›äº†ï¼š

- **å¼‚æ­¥çˆ¬è™«**: é«˜æ€§èƒ½å¼‚æ­¥ç½‘é¡µçˆ¬å–
- **æµè§ˆå™¨é…ç½®**: æ”¯æŒå¤šç§æµè§ˆå™¨å’Œé…ç½®é€‰é¡¹
- **æ™ºèƒ½æå–**: åŸºäºLLMçš„å†…å®¹æå–ç­–ç•¥
- **Markdownè½¬æ¢**: è‡ªåŠ¨å°†HTMLè½¬æ¢ä¸ºMarkdown
- **å¤šç§æå–ç­–ç•¥**: CSSé€‰æ‹©å™¨ã€XPathã€LLMç­‰

## ğŸ“¦ å®‰è£…å’Œé…ç½®

### 1. å®‰è£…Crawl4AI
```bash
# å®‰è£…crawl4ai
pip install crawl4ai==0.3.70

# å®‰è£…playwrightä¾èµ–
pip install playwright==1.40.0

# è¿è¡Œè®¾ç½®è„šæœ¬
crawl4ai-setup
```

### 2. Dockerç¯å¢ƒ
å¦‚æœä½¿ç”¨Dockerï¼Œcrawl4aiå·²ç»åŒ…å«åœ¨Dockerfileä¸­ï¼š
```bash
# æ„å»ºåŒ…å«crawl4aiçš„é•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d
```

## ğŸ”§ é…ç½®è¯´æ˜

### åŸºæœ¬é…ç½®
```yaml
web_crawler:
  config:
    browser:
      browser_type: "chromium"  # chromium, firefox, webkit
      headless: true
      viewport:
        width: 1920
        height: 1080
      user_agent: "Financial Data Collector 1.0"
    
    extraction_strategy: "llm"  # llm, css, xpath
    request_delay: 1.0
    max_concurrent_requests: 5
    timeout: 30
```

### LLMæå–ç­–ç•¥
```yaml
llm_config:
  provider: "openai"  # openai, anthropic, local
  model: "gpt-4"
  api_token: "${OPENAI_API_KEY}"
  instruction: "Extract financial data from the webpage"
  schema:
    type: "object"
    properties:
      stock_data:
        type: "object"
        properties:
          symbol: {"type": "string"}
          price: {"type": "number"}
          volume: {"type": "number"}
```

### CSSæå–ç­–ç•¥
```yaml
financial_selectors:
  price: [".price", ".quote-price", "[data-testid*='price']"]
  volume: [".volume", ".quote-volume", "[data-testid*='volume']"]
  change: [".change", ".quote-change", "[data-testid*='change']"]
  market_cap: [".market-cap", ".market-value", "[data-testid*='market-cap']"]
  news: [".news-item", ".article", ".story"]
```

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨
```python
from financial_data_collector.core.crawler.web_crawler import WebCrawler

# åˆ›å»ºçˆ¬è™«å®ä¾‹
crawler = WebCrawler("MyCrawler")

# åˆå§‹åŒ–é…ç½®
config = {
    "browser": {"headless": True},
    "extraction_strategy": "llm",
    "llm_config": {
        "provider": "openai",
        "model": "gpt-4",
        "api_token": "your-api-key"
    }
}
crawler.initialize(config)

# å¯åŠ¨çˆ¬è™«
await crawler.start()

# çˆ¬å–ç½‘é¡µ
result = await crawler.crawl_url(
    "https://finance.yahoo.com/quote/AAPL",
    {
        "extraction_strategy": "llm",
        "wait_for": 3,
        "max_scrolls": 2
    }
)

# åœæ­¢çˆ¬è™«
await crawler.stop()
```

### é«˜çº§é…ç½®
```python
# é…ç½®æµè§ˆå™¨é€‰é¡¹
browser_config = {
    "browser_type": "chromium",
    "headless": False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
    "viewport": {"width": 1920, "height": 1080},
    "user_agent": "Custom User Agent",
    "proxy": "http://proxy:8080",  # ä»£ç†è®¾ç½®
    "cookies": [{"name": "session", "value": "abc123"}]
}

# é…ç½®æå–ç­–ç•¥
extraction_config = {
    "extraction_strategy": "llm",
    "llm_config": {
        "provider": "openai",
        "model": "gpt-4",
        "api_token": "your-api-key",
        "instruction": "Extract financial data including stock prices, volume, and news",
        "schema": {
            "type": "object",
            "properties": {
                "stock_data": {"type": "object"},
                "news": {"type": "array"},
                "company_info": {"type": "object"}
            }
        }
    }
}

# é…ç½®çˆ¬å–é€‰é¡¹
crawl_options = {
    "wait_for": 3,  # ç­‰å¾…é¡µé¢åŠ è½½
    "max_scrolls": 5,  # æœ€å¤§æ»šåŠ¨æ¬¡æ•°
    "word_count_threshold": 100,  # æœ€å°å­—æ•°é˜ˆå€¼
    "cache_mode": "BYPASS"  # ç¼“å­˜æ¨¡å¼
}
```

## ğŸ¯ é‡‘èæ•°æ®çˆ¬å–ç¤ºä¾‹

### 1. è‚¡ç¥¨æ•°æ®çˆ¬å–
```python
# çˆ¬å–Yahoo Financeè‚¡ç¥¨é¡µé¢
stock_data = await crawler.crawl_url(
    "https://finance.yahoo.com/quote/AAPL",
    {
        "extraction_strategy": "css",
        "css_selectors": {
            "price": "[data-field='regularMarketPrice']",
            "volume": "[data-field='regularMarketVolume']",
            "change": "[data-field='regularMarketChange']",
            "market_cap": "[data-field='marketCap']"
        },
        "wait_for": 2
    }
)
```

### 2. æ–°é—»æ•°æ®çˆ¬å–
```python
# çˆ¬å–é‡‘èæ–°é—»
news_data = await crawler.crawl_url(
    "https://finance.yahoo.com/news/",
    {
        "extraction_strategy": "llm",
        "llm_config": {
            "provider": "openai",
            "model": "gpt-4",
            "instruction": "Extract financial news headlines, summaries, and timestamps"
        },
        "wait_for": 3,
        "max_scrolls": 3
    }
)
```

### 3. æ‰¹é‡çˆ¬å–
```python
# çˆ¬å–å¤šä¸ªè‚¡ç¥¨é¡µé¢
urls = [
    "https://finance.yahoo.com/quote/AAPL",
    "https://finance.yahoo.com/quote/MSFT",
    "https://finance.yahoo.com/quote/GOOGL"
]

results = await crawler.crawl_multiple_urls(
    urls,
    {
        "extraction_strategy": "css",
        "css_selectors": {
            "price": "[data-field='regularMarketPrice']",
            "symbol": "[data-field='symbol']"
        }
    }
)
```

## ğŸ” æ•°æ®æå–ç­–ç•¥

### 1. LLMæå–ç­–ç•¥
é€‚ç”¨äºå¤æ‚çš„å†…å®¹æå–ï¼Œéœ€è¦AIç†è§£ï¼š
```python
llm_strategy = {
    "extraction_strategy": "llm",
    "llm_config": {
        "provider": "openai",
        "model": "gpt-4",
        "instruction": "Extract all financial metrics and news from the page",
        "schema": {
            "type": "object",
            "properties": {
                "stock_metrics": {
                    "type": "object",
                    "properties": {
                        "price": {"type": "number"},
                        "volume": {"type": "number"},
                        "market_cap": {"type": "number"}
                    }
                },
                "news_items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "summary": {"type": "string"},
                            "timestamp": {"type": "string"}
                        }
                    }
                }
            }
        }
    }
}
```

### 2. CSSæå–ç­–ç•¥
é€‚ç”¨äºç»“æ„åŒ–æ•°æ®æå–ï¼š
```python
css_strategy = {
    "extraction_strategy": "css",
    "css_selectors": {
        "price": ".quote-price, [data-testid*='price']",
        "volume": ".quote-volume, [data-testid*='volume']",
        "news": ".news-item, .article",
        "company_name": ".company-name, h1"
    }
}
```

### 3. æ··åˆç­–ç•¥
ç»“åˆå¤šç§æå–æ–¹æ³•ï¼š
```python
mixed_strategy = {
    "extraction_strategy": "css",  # åŸºç¡€CSSæå–
    "css_selectors": {
        "basic_data": ".quote-data"
    },
    "llm_config": {  # å¯¹æå–çš„å†…å®¹è¿›è¡ŒAIåˆ†æ
        "provider": "openai",
        "model": "gpt-4",
        "instruction": "Analyze the extracted financial data and provide insights"
    }
}
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘æ§åˆ¶
```python
config = {
    "max_concurrent_requests": 10,  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    "request_delay": 1.0,  # è¯·æ±‚é—´éš”
    "timeout": 30  # è¯·æ±‚è¶…æ—¶
}
```

### 2. ç¼“å­˜ç­–ç•¥
```python
cache_options = {
    "cache_mode": "BYPASS",  # ç»•è¿‡ç¼“å­˜
    "cache_ttl": 3600  # ç¼“å­˜ç”Ÿå­˜æ—¶é—´
}
```

### 3. èµ„æºç®¡ç†
```python
browser_config = {
    "headless": True,  # æ— å¤´æ¨¡å¼èŠ‚çœèµ„æº
    "viewport": {"width": 1920, "height": 1080},
    "memory_limit": "2GB"  # å†…å­˜é™åˆ¶
}
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æµè§ˆå™¨å¯åŠ¨å¤±è´¥
```bash
# é‡æ–°å®‰è£…æµè§ˆå™¨ä¾èµ–
crawl4ai-setup

# æ£€æŸ¥ç³»ç»Ÿä¾èµ–
apt-get update && apt-get install -y chromium-browser
```

#### 2. LLM APIè°ƒç”¨å¤±è´¥
```python
# æ£€æŸ¥APIå¯†é’¥
config = {
    "llm_config": {
        "provider": "openai",
        "api_token": "your-valid-api-key",
        "model": "gpt-4"
    }
}
```

#### 3. é¡µé¢åŠ è½½è¶…æ—¶
```python
# å¢åŠ ç­‰å¾…æ—¶é—´
options = {
    "wait_for": 5,  # å¢åŠ ç­‰å¾…æ—¶é—´
    "timeout": 60,  # å¢åŠ è¶…æ—¶æ—¶é—´
    "max_scrolls": 0  # å‡å°‘æ»šåŠ¨æ¬¡æ•°
}
```

#### 4. å†…å­˜ä¸è¶³
```python
# ä¼˜åŒ–æµè§ˆå™¨é…ç½®
browser_config = {
    "headless": True,
    "memory_limit": "1GB",
    "disable_images": True,  # ç¦ç”¨å›¾ç‰‡åŠ è½½
    "disable_css": True  # ç¦ç”¨CSSåŠ è½½
}
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### 2. ä¿å­˜é¡µé¢æˆªå›¾
```python
options = {
    "screenshot": True,
    "screenshot_path": "/tmp/screenshot.png"
}
```

#### 3. ä¿å­˜HTMLå†…å®¹
```python
result = await crawler.crawl_url(url, options)
print("HTML content:", result.html)
print("Markdown content:", result.markdown)
```

## ğŸ“Š ç›‘æ§å’ŒæŒ‡æ ‡

### å¥åº·æ£€æŸ¥
```python
health = await crawler.health_check()
print(f"Status: {health['status']}")
print(f"Details: {health['details']}")
```

### æ€§èƒ½æŒ‡æ ‡
```python
# è·å–çˆ¬å–ç»Ÿè®¡
stats = crawler.get_crawl_stats()
print(f"Total requests: {stats['total_requests']}")
print(f"Success rate: {stats['success_rate']}")
print(f"Average response time: {stats['avg_response_time']}")
```

## ğŸ”’ å®‰å…¨å’Œåˆè§„

### 1. éµå®ˆrobots.txt
```python
config = {
    "respect_robots_txt": True,
    "user_agent": "Financial Data Collector 1.0"
}
```

### 2. è¯·æ±‚é¢‘ç‡æ§åˆ¶
```python
config = {
    "request_delay": 2.0,  # 2ç§’é—´éš”
    "max_concurrent_requests": 3  # é™åˆ¶å¹¶å‘
}
```

### 3. ä»£ç†ä½¿ç”¨
```python
browser_config = {
    "proxy": "http://proxy:8080",
    "proxy_auth": {"username": "user", "password": "pass"}
}
```

## ğŸ“š æœ€ä½³å®è·µ

1. **åˆç†ä½¿ç”¨LLM**: åªåœ¨éœ€è¦å¤æ‚ç†è§£æ—¶ä½¿ç”¨LLMæå–
2. **ç¼“å­˜ç­–ç•¥**: å¯¹ä¸ç»å¸¸å˜åŒ–çš„æ•°æ®ä½¿ç”¨ç¼“å­˜
3. **é”™è¯¯å¤„ç†**: å®ç°å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
4. **èµ„æºç®¡ç†**: åŠæ—¶å…³é—­æµè§ˆå™¨å®ä¾‹é‡Šæ”¾èµ„æº
5. **åˆè§„æ€§**: éµå®ˆç½‘ç«™çš„robots.txtå’Œä½¿ç”¨æ¡æ¬¾

## ğŸš€ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰æå–å™¨
```python
class CustomExtractor:
    def extract(self, html, url):
        # è‡ªå®šä¹‰æå–é€»è¾‘
        return extracted_data
```

### 2. æ•°æ®åå¤„ç†
```python
def post_process_data(data):
    # æ•°æ®æ¸…æ´—å’Œè½¬æ¢
    return cleaned_data
```

### 3. å®æ—¶ç›‘æ§
```python
# è®¾ç½®ç›‘æ§å›è°ƒ
def on_crawl_complete(result):
    print(f"Crawl completed: {result.url}")

crawler.set_callback("complete", on_crawl_complete)
```

é€šè¿‡ä»¥ä¸Šé…ç½®å’Œä½¿ç”¨æ–¹æ³•ï¼Œæ‚¨å¯ä»¥åœ¨Financial Data Collectorç³»ç»Ÿä¸­å……åˆ†åˆ©ç”¨Crawl4AIçš„å¼ºå¤§åŠŸèƒ½ï¼Œå®ç°é«˜æ•ˆã€æ™ºèƒ½çš„é‡‘èæ•°æ®çˆ¬å–ã€‚
